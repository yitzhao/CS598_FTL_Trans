{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "path = '/content/drive/MyDrive/FTL-Trans'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYLW0IvTrMIj",
        "outputId": "9e34b9f3-7dd8-4ef7-ac70-8e11fd30a58a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "haOSEA3m6YbC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adm = pd.read_csv('DATA/ADMISSIONS.csv')\n",
        "df_notes = pd.read_csv('DATA/NOTEEVENTS.csv')\n",
        "\n",
        "df_readmission = df_adm.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOVo1bdqTdF",
        "outputId": "32d4548e-497d-4d2f-c2ec-85dc78633b6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b9cca7a654e3>:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_notes = pd.read_csv('DATA/NOTEEVENTS.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_readmission.ADMITTIME = pd.to_datetime(df_readmission.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "df_readmission.DISCHTIME = pd.to_datetime(df_readmission.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "df_readmission.DEATHTIME = pd.to_datetime(df_readmission.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "df_readmission = df_readmission.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
        "df_readmission = df_readmission.reset_index(drop = True)\n",
        "df_readmission['NEXT_ADMITTIME'] = df_readmission.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
        "df_readmission['NEXT_ADMISSION_TYPE'] = df_readmission.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
        "rows = df_readmission.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
        "df_readmission.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
        "df_readmission.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN\n",
        "df_readmission = df_readmission.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
        "#When we filter out the \"ELECTIVE\", we need to correct the next admit time for these admissions since there might be 'emergency' next admit after \"ELECTIVE\"\n",
        "df_readmission[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = df_readmission.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
        "df_readmission['DAYS_NEXT_ADMIT']=  (df_readmission.NEXT_ADMITTIME - df_readmission.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
        "df_readmission['OUTPUT_LABEL'] = (df_readmission.DAYS_NEXT_ADMIT < 30).astype('int')\n",
        "### filter out newborn and death\n",
        "df_readmission = df_readmission[df_readmission['ADMISSION_TYPE']!='NEWBORN']\n",
        "df_readmission = df_readmission[df_readmission.DEATHTIME.isnull()]\n",
        "df_readmission['DURATION'] = (df_readmission['DISCHTIME']-df_readmission['ADMITTIME']).dt.total_seconds()/(24*60*60)"
      ],
      "metadata": {
        "id": "NeQaTfWhqW-a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = df_readmission[df_readmission['OUTPUT_LABEL']==1]['SUBJECT_ID'].unique()\n",
        "neg = df_readmission[df_readmission['OUTPUT_LABEL']==0]['SUBJECT_ID'].unique()\n",
        "neg = neg[~np.isin(neg,list(pos))]\n",
        "random_state = np.random.RandomState(1)\n",
        "pos = np.random.choice(pos, size = 2000, replace = False)\n",
        "neg = np.random.choice(neg, size = 2000, replace = False)\n",
        "pos_df = df_readmission[df_readmission['SUBJECT_ID'].isin(pos)]\n",
        "neg_df = df_readmission[df_readmission['SUBJECT_ID'].isin(neg)]\n",
        "df_readmission = pd.concat([pos_df,neg_df])\n",
        "df_readmission = df_readmission.merge(df_notes, on = ['HADM_ID'])\n",
        "df_readmission = df_readmission[['HADM_ID','ROW_ID_y','CHARTDATE','CHARTTIME','TEXT','OUTPUT_LABEL']]"
      ],
      "metadata": {
        "id": "ATvtUs_Rqel4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_readmission.to_csv('DATA/readmission.csv',index=False,header=None)"
      ],
      "metadata": {
        "id": "G6WxkPk1ql0n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers\n",
        "from pytorch_transformers import BertTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuDUqWZF7YoN",
        "outputId": "6492682c-c13b-40e1-cd96-8d0310ad89b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m174.1/176.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.25.2)\n",
            "Collecting boto3 (from pytorch_transformers)\n",
            "  Downloading boto3-1.34.84-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch_transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting botocore<1.35.0,>=1.34.84 (from boto3->pytorch_transformers)\n",
            "  Downloading botocore-1.34.84-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_transformers)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.84->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.84->boto3->pytorch_transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_transformers\n",
            "Successfully installed boto3-1.34.84 botocore-1.34.84 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch_transformers-1.2.0 s3transfer-0.10.1 sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 preprocessing.py --original_data DATA/readmission.csv --output_dir DATA/raw_readmission/ --temp_dir DATA/readmission_temp/ --task_name readmission_FTL_Trans --log_path log/preprocess_readmission.txt --id_num_neg 2000 --id_num_pos 2000 --random_seed 1 --bert_model bert-base-uncased"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ3sHGSWq0sl",
        "outputId": "1b58b4e2-9814-4068-b78e-30996bc90b6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 231508/231508 [00:00<00:00, 426077.53B/s]\n",
            "New Pre-processing Job Start! \n",
            "original_data: DATA/readmission.csv, output_dir: DATA/raw_readmission/, temp_dir: DATA/readmission_temp/ \n",
            "task_name: readmission_FTL_Trans, log_path: log/preprocess_readmission.txt\n",
            "id_num_neg: 2000, id_num_pos: 2000\n",
            "random_seed: 1, bert_model: bert-base-uncased\n",
            "chunk 0 tokenize start!\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 split_into_chunk.py --data_dir DATA/raw_readmission/ --train_data train.csv --val_data val.csv --test_data test.csv --log_path log/split_readmission.txt --output_dir DATA/readmission/ --max_seq_length 128"
      ],
      "metadata": {
        "id": "JKho7A45q04j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_clbert_ftlstm.py --data_dir ./DATA/readmission/ --train_data train.csv --val_data val.csv --test_data test.csv --log_path ./log_readmission.txt --bert_model ./pretraining/ --embed_mode all --task_name FTL-Trans_Prediction --max_seq_length 128 --train_batch_size 32 --eval_batch_size 1 --learning_rate 2e-5 --num_train_epochs 3 --warmup_proportion 0.1 --max_chunk_num 32 --seed 42 --gradient_accumulation_steps 1 --output_dir ./exp_FTL-Trans --save_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgartBrCsqUb",
        "outputId": "26759c35-fa72-4b24-8d79-2b9da686ebbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in the modeling class\n",
            "04/08/2024 03:07:22 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n",
            "New Job Start! \n",
            "Data directory: ./DATA/readmission/, Directory Code: ./DATA/readmission/, Save Model: True\n",
            "Output_dir: ./exp_FTL-Trans, Task Name: FTL-Trans_Prediction, embed_mode: all\n",
            "max_seq_length: 128,  max_chunk_num: 32\n",
            "train_batch_size: 32, eval_batch_size: 1\n",
            "learning_rate: 2e-05, warmup_proportion: 0.1\n",
            "num_train_epochs: 3, seed: 42, gradient_accumulation_steps: 1\n",
            "FTLSTM Model's lstm_layers: 1\n",
            "config setting: \n",
            "hidden_dropout_prob: 0.1 \n",
            "layer_norm_eps: 1e-12 \n",
            "initializer_range: 0.02 \n",
            "max_note_position_embedding: 1000 \n",
            "max_chunk_position_embedding: 1000 \n",
            "embed_mode: all \n",
            "hidden_size: 768 \n",
            "lstm_layers: 1 \n",
            "task_name: FTL-Trans_Prediction \n",
            "\n",
            "Number of GPU is 1\n",
            "Device Name: Tesla V100-SXM2-16GB,Device Capability: (7, 0)\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   Model name './pretraining/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './pretraining/' is a path or url to a directory containing tokenizer files.\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./pretraining/added_tokens.json. We won't load it.\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./pretraining/special_tokens_map.json. We won't load it.\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./pretraining/tokenizer_config.json. We won't load it.\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   loading file ./pretraining/vocab.txt\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "04/08/2024 03:07:27 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "Tokenize Start!\n",
            "Tokenize Finished!\n",
            "train dataset size is 381511,\n",
            "validation dataset size is 43775,\n",
            "test dataset size is 61370\n",
            "04/08/2024 03:08:26 - INFO - modeling_readmission -   loading archive file ./pretraining/\n",
            "04/08/2024 03:08:26 - INFO - modeling_readmission -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Training start!\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "Train loss: 0.6929972989851212\n",
            "Validation Accuracy: 0.4986737400530504\n",
            "Epoch:  33% 1/3 [16:07<32:15, 967.55s/it]Train loss: 0.6931471824645996\n",
            "Validation Accuracy: 0.4986737400530504\n",
            "Epoch:  67% 2/3 [32:19<16:10, 970.33s/it]Train loss: 0.6931471824645996\n",
            "Validation Accuracy: 0.4986737400530504\n",
            "Epoch: 100% 3/3 [48:30<00:00, 970.02s/it]\n",
            "total training time is: 2910.0690009593964s\n",
            "Model saved!\n",
            "Test Patient Level Accuracy: 0.503957783641161\n",
            "Test Patient Level F1 Score: 0.6701754385964912\n",
            "Test Patient Level Precision: 0.503957783641161\n",
            "Test Patient Level Recall: 1.0\n",
            "Test Patient Level AUC: 0.5 \n",
            "Test Patient Level Matthew's correlation coefficient: 0.0\n",
            "Test Patient Level AUPR: 0.7519788918205805 \n",
            "All Finished!\n"
          ]
        }
      ]
    }
  ]
}